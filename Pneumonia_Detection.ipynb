{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjgVy88FEZhe"
      },
      "source": [
        "##Importing Dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "Nfe9YdipD1tS",
        "outputId": "2419b840-f839-47d6-cdaa-d90250bd63b7"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"dataset.zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "BadZipFile",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0b4c4ab59fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dataset.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrwxFHFQIcs3"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1RJTxzyKXhx"
      },
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uERTZRqVKtbn"
      },
      "source": [
        "cp kaggle.json  ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUgiDwx_K2ex"
      },
      "source": [
        "!chmod 600  ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV7CpLsmLj_z",
        "outputId": "53c79f43-d100-4ee5-d272-621495d0437a"
      },
      "source": [
        "!kaggle datasets download -d tolgadincer/labeled-chest-xray-images "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading labeled-chest-xray-images.zip to /content\n",
            "100% 1.17G/1.17G [00:17<00:00, 107MB/s]\n",
            "100% 1.17G/1.17G [00:17<00:00, 73.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQLDRphDOPlv"
      },
      "source": [
        "!unzip labeled-chest-xray-images.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Pvw-a9xJsB"
      },
      "source": [
        "##Starting Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP2k3irNEHAV"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZI8KDfVEJdo",
        "outputId": "a95f4995-9873-4ae1-8644-0cb6dc194f55"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        'chest_xray/train',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5232 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHPW9S3nELz9",
        "outputId": "656a37b9-aaa2-4d2f-c029-d32d5c75611d"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'chest_xray/test',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4cSzxRHEQDS"
      },
      "source": [
        "cnn=tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nWLFxFlETgX"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSjrVVEBEb49"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz4OeWYmEeLU"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xuAUVZzEgiG"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_KO6ajjEjA0"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in4e_r0OEmCH"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSLjw6veEpOw"
      },
      "source": [
        "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvjYQLxoErnc",
        "outputId": "9b22ee74-6de0-4114-eb7e-1b7826b08006"
      },
      "source": [
        "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "164/164 [==============================] - 83s 498ms/step - loss: 0.3824 - accuracy: 0.8347 - val_loss: 0.2527 - val_accuracy: 0.9038\n",
            "Epoch 2/25\n",
            "164/164 [==============================] - 80s 488ms/step - loss: 0.2346 - accuracy: 0.9044 - val_loss: 0.3148 - val_accuracy: 0.8702\n",
            "Epoch 3/25\n",
            "164/164 [==============================] - 80s 490ms/step - loss: 0.2021 - accuracy: 0.9178 - val_loss: 0.3007 - val_accuracy: 0.8734\n",
            "Epoch 4/25\n",
            "164/164 [==============================] - 81s 494ms/step - loss: 0.1751 - accuracy: 0.9304 - val_loss: 0.1692 - val_accuracy: 0.9343\n",
            "Epoch 5/25\n",
            "164/164 [==============================] - 80s 489ms/step - loss: 0.1751 - accuracy: 0.9310 - val_loss: 0.2282 - val_accuracy: 0.9119\n",
            "Epoch 6/25\n",
            "164/164 [==============================] - 80s 487ms/step - loss: 0.1598 - accuracy: 0.9379 - val_loss: 0.2644 - val_accuracy: 0.8894\n",
            "Epoch 7/25\n",
            "164/164 [==============================] - 80s 489ms/step - loss: 0.1539 - accuracy: 0.9360 - val_loss: 0.3711 - val_accuracy: 0.8622\n",
            "Epoch 8/25\n",
            "164/164 [==============================] - 81s 491ms/step - loss: 0.1465 - accuracy: 0.9442 - val_loss: 0.1477 - val_accuracy: 0.9439\n",
            "Epoch 9/25\n",
            "164/164 [==============================] - 80s 487ms/step - loss: 0.1377 - accuracy: 0.9453 - val_loss: 0.1987 - val_accuracy: 0.9311\n",
            "Epoch 10/25\n",
            "164/164 [==============================] - 80s 487ms/step - loss: 0.1335 - accuracy: 0.9469 - val_loss: 0.1438 - val_accuracy: 0.9471\n",
            "Epoch 11/25\n",
            "164/164 [==============================] - 81s 494ms/step - loss: 0.1381 - accuracy: 0.9484 - val_loss: 0.1719 - val_accuracy: 0.9343\n",
            "Epoch 12/25\n",
            "164/164 [==============================] - 80s 489ms/step - loss: 0.1307 - accuracy: 0.9503 - val_loss: 0.3086 - val_accuracy: 0.8942\n",
            "Epoch 13/25\n",
            "164/164 [==============================] - 80s 487ms/step - loss: 0.1281 - accuracy: 0.9528 - val_loss: 0.2096 - val_accuracy: 0.9215\n",
            "Epoch 14/25\n",
            "164/164 [==============================] - 79s 482ms/step - loss: 0.1304 - accuracy: 0.9497 - val_loss: 0.1748 - val_accuracy: 0.9343\n",
            "Epoch 15/25\n",
            "164/164 [==============================] - 79s 480ms/step - loss: 0.1156 - accuracy: 0.9539 - val_loss: 0.1499 - val_accuracy: 0.9487\n",
            "Epoch 16/25\n",
            "164/164 [==============================] - 80s 485ms/step - loss: 0.1112 - accuracy: 0.9566 - val_loss: 0.1427 - val_accuracy: 0.9407\n",
            "Epoch 17/25\n",
            "164/164 [==============================] - 80s 485ms/step - loss: 0.1200 - accuracy: 0.9518 - val_loss: 0.2431 - val_accuracy: 0.9151\n",
            "Epoch 18/25\n",
            "164/164 [==============================] - 79s 483ms/step - loss: 0.1085 - accuracy: 0.9589 - val_loss: 0.1512 - val_accuracy: 0.9423\n",
            "Epoch 19/25\n",
            "164/164 [==============================] - 80s 485ms/step - loss: 0.1095 - accuracy: 0.9604 - val_loss: 0.2482 - val_accuracy: 0.9151\n",
            "Epoch 20/25\n",
            "164/164 [==============================] - 80s 485ms/step - loss: 0.1073 - accuracy: 0.9625 - val_loss: 0.2035 - val_accuracy: 0.9375\n",
            "Epoch 21/25\n",
            "164/164 [==============================] - 80s 485ms/step - loss: 0.0972 - accuracy: 0.9631 - val_loss: 0.1593 - val_accuracy: 0.9407\n",
            "Epoch 22/25\n",
            "164/164 [==============================] - 80s 486ms/step - loss: 0.1026 - accuracy: 0.9623 - val_loss: 0.2067 - val_accuracy: 0.9231\n",
            "Epoch 23/25\n",
            "164/164 [==============================] - 79s 483ms/step - loss: 0.0989 - accuracy: 0.9620 - val_loss: 0.1494 - val_accuracy: 0.9391\n",
            "Epoch 24/25\n",
            "164/164 [==============================] - 79s 479ms/step - loss: 0.0992 - accuracy: 0.9627 - val_loss: 0.1581 - val_accuracy: 0.9391\n",
            "Epoch 25/25\n",
            "164/164 [==============================] - 78s 477ms/step - loss: 0.1038 - accuracy: 0.9589 - val_loss: 0.1706 - val_accuracy: 0.9327\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6e4efa7fd0>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k02UAl4SH_QZ"
      },
      "source": [
        "## Part 4 - Making a single prediction After Some Change More Accurate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNOmipX_HLkx"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image=image.load_img('dataset/single_prediction/cat_or_dog_1.jpg',target_size=(64,64))\n",
        "test_image=image.img_to_array(test_image)\n",
        "######## Add by me ###### feature scaling\n",
        "test_image=test_image/255.0\n",
        "#########################\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "result=cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "print('result ')\n",
        "print(result[0][0])\n",
        "##### given by sir ######\n",
        "#if result[0][0] ==1:   \n",
        "#  prediction='dog'\n",
        "#else:\n",
        "#  prediction='cat' \n",
        "#########################\n",
        "\n",
        "###### Add by me ####\n",
        "if result[0][0] >0.5:\n",
        "  prediction='dog'\n",
        "else:\n",
        "  prediction='cat' \n",
        "#####################  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEihJS5DHOQR"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQU-ayZsHSXG"
      },
      "source": [
        "##Saving our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nR8XxobHRFz"
      },
      "source": [
        "cnn.save('cat_or_dog_saved_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAIu7EW5HYrC"
      },
      "source": [
        "from keras.models import load_model\n",
        "new_model=load_model('cat_or_dog_saved_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B60RP0LAHdKH"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exehRHzRHfI_"
      },
      "source": [
        "new_model.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qy7yCvxHg6M"
      },
      "source": [
        "new_model.optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZLu9wGfIFqh"
      },
      "source": [
        "##Prediction through Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaZHIS-OIIQt"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image=image.load_img('cat_or_dog_15.jpeg',target_size=(64,64))\n",
        "test_image=image.img_to_array(test_image)\n",
        "######## Add by me ###### feature scaling\n",
        "test_image=test_image/255.0\n",
        "#########################\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "result=new_model.predict(test_image)\n",
        "#training_set.class_indices. ## for determining which index belongs to which class like (Dog =1,Cat =0)\n",
        "print('result ')\n",
        "print(result[0][0])\n",
        "##### given by sir ######\n",
        "#if result[0][0] ==1:   \n",
        "#  prediction='dog'\n",
        "#else:\n",
        "#  prediction='cat' \n",
        "#########################\n",
        "\n",
        "###### Add by me ####\n",
        "if result[0][0] >0.5:\n",
        "  prediction='dog'\n",
        "else:\n",
        "  prediction='cat' \n",
        "#####################  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkJxYVYoIRDR"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOvos62DIVDf"
      },
      "source": [
        "##Creating TFlite from saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Iq35RFIWva"
      },
      "source": [
        "from tensorflow import lite\n",
        "converter=lite.TFLiteConverter.from_keras_model(new_model)\n",
        "tfmodel=converter.convert()\n",
        "open('model.tflite','wb').write(tfmodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbfMBGEAIftn"
      },
      "source": [
        "##Checking our TFlite model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwYfEvzgIj4K"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "img=image.load_img('cat_or_dog_15.jpeg',target_size=(64,64))\n",
        "img=image.img_to_array(img)\n",
        "######## Add by me ###### feature scaling\n",
        "img=img/255.0\n",
        "#########################\n",
        "img=np.expand_dims(img,axis=0)\n",
        "\n",
        "#img = cv2.imread(\"cat_or_dog_17.jpg\")\n",
        "#img = cv2.resize(img, (64,64))\n",
        "#img = np.array(img, dtype=\"float32\")\n",
        "#img = np.reshape(img, (1,64,64,3))\n",
        "\n",
        "\n",
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "print(\"*\"*50, input_details)\n",
        "interpreter.set_tensor(input_details[0]['index'], img)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}